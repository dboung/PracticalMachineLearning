---
title: "PracticeMachineLearningAssignment"
author: "Disovankiri Boung"
date: "8/8/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Practice Machine Learning

Using devices such as Jawbone Up, Nike FuelBand, and Fitbit it is now possible to collect a large amount of data about personal activity relatively inexpensively. These type of devices are part of the quantified self movement â€“ a group of enthusiasts who take measurements about themselves regularly to improve their health, to find patterns in their behavior, or because they are tech geeks. One thing that people regularly do is quantify how much of a particular activity they do, but they rarely quantify how well they do it. In this project, your goal will be to use data from accelerometers on the belt, forearm, arm, and dumbell of 6 participants. They were asked to perform barbell lifts correctly and incorrectly in 5 different ways. More information is available from the website here: http://groupware.les.inf.puc-rio.br/har (see the section on the Weight Lifting Exercise Dataset).


## Data
The data for this project comes from:

Velloso, E.; Bulling, A.; Gellersen, H.; Ugulino, W.; Fuks, H. Qualitative Activity Recognition of Weight Lifting Exercises. Proceedings of 4th International Conference in Cooperation with SIGCHI (Augmented Human '13) . Stuttgart, Germany: ACM SIGCHI, 2013.

Read more: http://groupware.les.inf.puc-rio.br/har#wle_paper_section#ixzz6UaDAwCTK

The training data for this project are available here: [https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv]

The test data are available here: [https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv]

## Summary

In this project, we used Machine Learning Technique to predict outcome variable which is classe, a factor variable consist of A,B,C,D,and E. Participants were asked to perform one set of 10 repetitions of the Unilateral Dumbbell Biceps Curl in 5 different fashions:

* exactly according to the specification (Class A)
* throwing the elbows to the front (Class B)
* lifting the dumbbell only halfway (Class C)
* lowering the dumbbell only halfway (Class D)
* throwing the hips to the front (Class E)

We use the following steps to do our analysis:

* To reproduce the result again, we set seed to be 497
* We split the training data into subtraning (70%), and subtesting (30%)
* We used the subtraining data to build 2 models ,namely, Decision Tree and RandomForest
* We cross-validate the model with the subtesting data before used it with the real testing data
* According to the subtesting dataset, RandomForest has higher accuracy rate than Decision Tree model


```{r ,message=FALSE,warning=FALSE}
library(lattice); library(ggplot2); library(caret); library(randomForest); library(rpart); library(rpart.plot); library(rattle)
```

### Importing Dataset
```{r,comment=NULL}
set.seed(497)
training <- read.csv("pml-training.csv", na.strings=c("NA","#DIV/0!", ""))
test <- read.csv("pml-testing.csv", na.strings=c("NA","#DIV/0!", ""))

dim(training); dim(test)

# drop column with all NA value
training <- training[,colSums(is.na(training)) == 0]
test <- test[,colSums(is.na(test)) == 0]


```
### Cross-Validation

In order to cross-validate our best fitted model, we split our training set into training (70% of training dataset) and testing (30% of training dataset).

```{r,comment=NULL}
#Split training dataset into Training Subsample, and Testing Subsample
subsample <- createDataPartition(y=training$classe,p=0.70,list=FALSE)
TrainingSub <- training[subsample,]
TestSub <- training[-subsample,]

#Removing the frist 7 columns which are not neccessary
TrainingSub <- TrainingSub[,-c(1:7)]
TestSub <- TestSub[,-c(1:7)]

dim(TrainingSub)

dim(TestSub)

```

### Prediction 1: Decision Tree
```{r,comment=NULL}
#building model
model1 <- rpart(classe~.,data=TrainingSub,method="class")

#prediction
predict1 <- predict(model1,TestSub,type="class")

#plot
fancyRpartPlot(model1)
```

```{r,comment=NULL}
confusionMatrix(predict1,TestSub$classe)
```
### Second Prediction: Random Forest
```{r,comment=NULL}
model2 <- randomForest(classe ~. , data=TrainingSub, method="class")

predict2 <- predict(model2, TestSub, type = "class")

confusionMatrix(predict2,TestSub$classe)
```
### Accuracy for Decision Tree and Random Forest
From above models, the accuracy of Decision Tree prediction model is 0.6923 is less than RandomForest prediction model which is 0.9969. **So, we choose RandomForest as our predicting model**.

### Applying RandomForest (Model2) to our Test Dataset
```{r, comment=""}
predict_final <- predict(model2, test, type="class")

predict_final
```

